# -*- coding: utf-8 -*-
"""digitdetector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12-g10dhZb14793Dbk0Z0e7iP6omEs0J-
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
# Path to dataset directory
data_path = '/kaggle/input/digit-recognizer/'

# Check files inside the dataset directory
for dirname, _, filenames in os.walk(data_path):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Load training data
df = pd.read_csv(os.path.join(data_path, 'train.csv'))

# Quick summary
print(df.info())

# Check for missing values
print(df.isnull().sum())

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import matplotlib.pyplot as plt

# Take first row (label + pixels)
sample = df.iloc[0, 1:].values.reshape(28, 28)  # reshape to 28x28 image
label = df.iloc[0, 0]

plt.imshow(sample, cmap='gray')
plt.title(f"Label: {label}")
plt.axis('off')
plt.show()

# Features = pixel values, Labels = digit
X = df.drop('label', axis=1).values
y = df['label'].values

print("X shape:", X.shape)  # (42000, 784)
print("y shape:", y.shape)  # (42000,)

X = X / 255.0

# For CNN
X = X.reshape(-1, 28, 28, 1)
print("Reshaped X:", X.shape)  # (42000, 28, 28, 1)

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Training samples:", X_train.shape[0])
print("Validation samples:", X_val.shape[0])

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    MaxPooling2D((2,2)),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D((2,2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')  # 10 digits (0–9)
])

from tensorflow.keras import Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model = Sequential([
    Input(shape=(28, 28, 1)),  # define input here
    Conv2D(32, (3,3), activation='relu'),
    MaxPooling2D((2,2)),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D((2,2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=128,
    validation_data=(X_val, y_val),
    verbose=2
)

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.show()

import pandas as pd

test_df = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')
X_test = test_df.values / 255.0  # normalize like train data
X_test = X_test.reshape(-1, 28, 28, 1)

# Predict on test data
y_pred = model.predict(X_test)   # probability distribution for each digit
y_pred_classes = y_pred.argmax(axis=1)  # get class index (0–9)

submission = pd.DataFrame({
    'ImageId': range(1, len(y_pred_classes) + 1),
    'Label': y_pred_classes
})

submission.to_csv('submission.csv', index=False)
print("✅ submission.csv created!")

from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.layers import Flatten, Dense, Dropout

model2 = Sequential([
    Input(shape=(28, 28, 1)),

    # Block 1
    Conv2D(32, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(32, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2,2)),
    Dropout(0.25),

    # Block 2
    Conv2D(64, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(64, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2,2)),
    Dropout(0.25),

    # Block 3
    Conv2D(128, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(128, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2,2)),
    Dropout(0.25),

    # Fully connected layers
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

# Compile model
model2.compile(optimizer='adam',
               loss='sparse_categorical_crossentropy',
               metrics=['accuracy'])

model2.summary()

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint('best_advanced_cnn.h5', monitor='val_loss', save_best_only=True)
]

history2 = model2.fit(
    X_train, y_train,
    epochs=30,
    batch_size=128,
    validation_data=(X_val, y_val),
    callbacks=callbacks,
    verbose=2
)